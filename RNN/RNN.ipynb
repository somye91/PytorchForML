{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53f5bf8c-ef2e-410e-af74-b932655a9da7",
   "metadata": {},
   "source": [
    "### Download the novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8ec87b9-b217-4ac9-9f0b-d500eace6c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url=\"https://gutenberg.org/cache/epub/73489/pg73489.txt\"\n",
    "response=requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(\"book.txt\", \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "else:\n",
    "    print(\"File not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33526b94-4053-4ef7-b17a-2f4855e89790",
   "metadata": {},
   "source": [
    "### Determine Start and End index of the text\n",
    "#### Prepare an index_to_char and char_to_index dictionary that will come in handy later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "20b018e6-60a7-4a79-a48c-21bc37db780d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of novel: 192221 \n",
      "\n",
      "Distinct characters: ['\\n', ' ', '!', '(', ')', '*', '+', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '£', '½', 'é', '‘', '’', '“', '”'] \n",
      "\n",
      "Count of distinct characters: 86 \n",
      "\n",
      "192221\n"
     ]
    }
   ],
   "source": [
    "with open(\"book.txt\",'r') as fp:\n",
    "    text=fp.read()\n",
    "start_idx=text.index(\"Once upon a time there was a little girl named Anne Wilbraham\")\n",
    "end_idx=text.index(\"THE END\")\n",
    "text=text[start_idx:end_idx]\n",
    "char_set=sorted(set(text))  ## Set function will prepare a list of unique characters in `text`\n",
    "print(f\"Total length of novel: {len(text)} \\n\")\n",
    "print(f\"Distinct characters: {char_set} \\n\")\n",
    "print(f\"Count of distinct characters: {len(char_set)} \\n\")\n",
    "\n",
    "char_to_idx = { char:idx for idx,char  in enumerate(char_set) }\n",
    "idx_to_char = { idx:char for idx,char  in enumerate(char_set) }\n",
    "\n",
    "## Convert original text to integers\n",
    "text_encoded = [ char_to_idx[char] for char in text ]\n",
    "print(len(text_encoded))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9627a98d-4447-4670-8cd0-daf771e5f4c4",
   "metadata": {},
   "source": [
    "### Prepare a custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0222d699-d2d3-4df7-8355-5dba9a18e24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([38, 65, 54, 56,  1, 72, 67, 66, 65,  1, 52,  1, 71, 60, 64, 56,  1, 71,\n",
      "        59, 56, 69, 56,  1, 74, 52, 70,  1, 52,  1, 63, 60, 71, 71, 63, 56,  1,\n",
      "        58, 60, 69, 63,  1, 65, 52, 64, 56, 55,  1, 24, 65, 65]) \n",
      " tensor(56)\n",
      "Once upon a time there was a little girl named Ann\n",
      "e\n",
      "192171\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PrepareData(Dataset):\n",
    "    def __init__(self,text_encoded, window_size=50):\n",
    "        self.text_encoded = text_encoded\n",
    "        self.window_size = window_size\n",
    "        self.input, self.label=self.getdata()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        input=self.input[idx]\n",
    "        label=self.label[idx]\n",
    "        return torch.tensor(input).long(), torch.tensor(label)\n",
    "\n",
    "    def getdata(self):\n",
    "        input=[]\n",
    "        label=[]\n",
    "        for i in range(len(self.text_encoded) - self.window_size):\n",
    "            input.append(self.text_encoded[i:i+self.window_size])\n",
    "            label.append(self.text_encoded[i+self.window_size])\n",
    "        return input,label\n",
    "                \n",
    "dataset=PrepareData(text_encoded)\n",
    "for i,j in dataset:\n",
    "    print(i, \"\\n\", j)\n",
    "    print(\"\".join([idx_to_char[a.item()] for a in i]))   ##Print the first input(sequence)\n",
    "    print(idx_to_char[j.item()])  ##Print the first label, i.e next character prediction\n",
    "    break\n",
    "print(len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c4a5386d-3e12-460d-8dfd-9bc247b177c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "torch.manual_seed(42)\n",
    "seq_dl = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "vocab_size = len(char_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "03a9a31f-cf96-4874-94de-ee893336622a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 86])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,10) ##  64, 50, 10\n",
    "        self.lstm = nn.LSTM(10,16, batch_first=True)  ## 64, 50, 16\n",
    "        self.linear = nn.Linear(16,vocab_size)\n",
    "\n",
    "    def forward(self,x, hidden=None):\n",
    "        embedded=self.embedding(x)\n",
    "        #print(embedded.shape)\n",
    "        output, (h_n, c_n) = self.lstm(embedded)\n",
    "        #print(output.shape)\n",
    "        output = output[:, -1, :] ## We only want the last timestep\n",
    "        #print(output.shape)\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        #output = self.linear(output) #.reshape(output.size(0), -1)\n",
    "        return output,h_n\n",
    "input,label=next(iter(seq_dl))\n",
    "rnn=RNN(vocab_size)\n",
    "rnn(input)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "da9cc1b0-95aa-410b-be2a-e591c9a3fc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Loss: 2.7632\n",
      "Epoch [2], Loss: 2.3698\n",
      "Epoch [3], Loss: 2.2650\n",
      "Epoch [4], Loss: 2.2036\n",
      "Epoch [5], Loss: 2.1644\n",
      "Epoch [6], Loss: 2.1352\n",
      "Epoch [7], Loss: 2.1127\n",
      "Epoch [8], Loss: 2.0943\n",
      "Epoch [9], Loss: 2.0787\n",
      "Epoch [10], Loss: 2.0659\n",
      "Epoch [11], Loss: 2.0548\n",
      "Epoch [12], Loss: 2.0451\n",
      "Epoch [13], Loss: 2.0367\n",
      "Epoch [14], Loss: 2.0294\n",
      "Epoch [15], Loss: 2.0226\n",
      "Epoch [16], Loss: 2.0166\n",
      "Epoch [17], Loss: 2.0115\n",
      "Epoch [18], Loss: 2.0067\n",
      "Epoch [19], Loss: 2.0027\n",
      "Epoch [20], Loss: 1.9988\n",
      "Epoch [21], Loss: 1.9951\n",
      "Epoch [22], Loss: 1.9919\n",
      "Epoch [23], Loss: 1.9890\n",
      "Epoch [24], Loss: 1.9857\n",
      "Epoch [25], Loss: 1.9829\n",
      "Epoch [26], Loss: 1.9806\n",
      "Epoch [27], Loss: 1.9784\n",
      "Epoch [28], Loss: 1.9755\n",
      "Epoch [29], Loss: 1.9730\n",
      "Epoch [30], Loss: 1.9708\n",
      "Epoch [31], Loss: 1.9689\n",
      "Epoch [32], Loss: 1.9665\n",
      "Epoch [33], Loss: 1.9648\n",
      "Epoch [34], Loss: 1.9626\n",
      "Epoch [35], Loss: 1.9607\n",
      "Epoch [36], Loss: 1.9587\n",
      "Epoch [37], Loss: 1.9570\n",
      "Epoch [38], Loss: 1.9549\n",
      "Epoch [39], Loss: 1.9535\n",
      "Epoch [40], Loss: 1.9518\n",
      "Epoch [41], Loss: 1.9495\n",
      "Epoch [42], Loss: 1.9488\n",
      "Epoch [43], Loss: 1.9466\n",
      "Epoch [44], Loss: 1.9452\n",
      "Epoch [45], Loss: 1.9438\n",
      "Epoch [46], Loss: 1.9427\n",
      "Epoch [47], Loss: 1.9413\n",
      "Epoch [48], Loss: 1.9401\n",
      "Epoch [49], Loss: 1.9390\n",
      "Epoch [50], Loss: 1.9376\n",
      "Epoch [51], Loss: 1.9363\n",
      "Epoch [52], Loss: 1.9351\n",
      "Epoch [53], Loss: 1.9343\n",
      "Epoch [54], Loss: 1.9353\n",
      "Epoch [55], Loss: 1.9323\n",
      "Epoch [56], Loss: 1.9312\n",
      "Epoch [57], Loss: 1.9302\n",
      "Epoch [58], Loss: 1.9294\n",
      "Epoch [59], Loss: 1.9284\n",
      "Epoch [60], Loss: 1.9276\n",
      "Epoch [61], Loss: 1.9265\n",
      "Epoch [62], Loss: 1.9260\n",
      "Epoch [63], Loss: 1.9252\n",
      "Epoch [64], Loss: 1.9245\n",
      "Epoch [65], Loss: 1.9238\n",
      "Epoch [66], Loss: 1.9226\n",
      "Epoch [67], Loss: 1.9227\n",
      "Epoch [68], Loss: 1.9216\n",
      "Epoch [69], Loss: 1.9207\n",
      "Epoch [70], Loss: 1.9201\n",
      "Epoch [71], Loss: 1.9197\n",
      "Epoch [72], Loss: 1.9185\n",
      "Epoch [73], Loss: 1.9223\n",
      "Epoch [74], Loss: 1.9192\n",
      "Epoch [75], Loss: 1.9161\n",
      "Epoch [76], Loss: 1.9162\n",
      "Epoch [77], Loss: 1.9163\n",
      "Epoch [78], Loss: 1.9157\n",
      "Epoch [79], Loss: 1.9150\n",
      "Epoch [80], Loss: 1.9143\n",
      "Epoch [81], Loss: 1.9143\n",
      "Epoch [82], Loss: 1.9135\n",
      "Epoch [83], Loss: 1.9139\n",
      "Epoch [84], Loss: 1.9122\n",
      "Epoch [85], Loss: 1.9119\n",
      "Epoch [86], Loss: 1.9116\n",
      "Epoch [87], Loss: 1.9116\n",
      "Epoch [88], Loss: 1.9104\n",
      "Epoch [89], Loss: 1.9099\n",
      "Epoch [90], Loss: 1.9113\n",
      "Epoch [91], Loss: 1.9107\n",
      "Epoch [92], Loss: 1.9088\n",
      "Epoch [93], Loss: 1.9104\n",
      "Epoch [94], Loss: 1.9079\n",
      "Epoch [95], Loss: 1.9078\n",
      "Epoch [96], Loss: 1.9073\n",
      "Epoch [97], Loss: 1.9066\n",
      "Epoch [98], Loss: 1.9060\n",
      "Epoch [99], Loss: 1.9058\n",
      "Epoch [100], Loss: 1.9051\n",
      "Epoch [101], Loss: 1.9053\n",
      "Epoch [102], Loss: 1.9045\n",
      "Epoch [103], Loss: 1.9044\n",
      "Epoch [104], Loss: 1.9041\n",
      "Epoch [105], Loss: 1.9038\n",
      "Epoch [106], Loss: 1.9033\n",
      "Epoch [107], Loss: 1.9032\n",
      "Epoch [108], Loss: 1.9020\n",
      "Epoch [109], Loss: 1.9020\n",
      "Epoch [110], Loss: 1.9032\n",
      "Epoch [111], Loss: 1.9011\n",
      "Epoch [112], Loss: 1.9057\n",
      "Epoch [113], Loss: 1.9115\n",
      "Epoch [114], Loss: 1.9037\n",
      "Epoch [115], Loss: 1.9004\n",
      "Epoch [116], Loss: 1.8997\n",
      "Epoch [117], Loss: 1.8998\n",
      "Epoch [118], Loss: 1.8992\n",
      "Epoch [119], Loss: 1.8995\n",
      "Epoch [120], Loss: 1.8983\n",
      "Epoch [121], Loss: 1.8993\n",
      "Epoch [122], Loss: 1.8982\n",
      "Epoch [123], Loss: 1.9000\n",
      "Epoch [124], Loss: 1.8972\n",
      "Epoch [125], Loss: 1.8970\n",
      "Epoch [126], Loss: 1.8971\n",
      "Epoch [127], Loss: 1.8971\n",
      "Epoch [128], Loss: 1.8966\n",
      "Epoch [129], Loss: 1.8965\n",
      "Epoch [130], Loss: 1.8962\n",
      "Epoch [131], Loss: 1.8962\n",
      "Epoch [132], Loss: 1.8956\n",
      "Epoch [133], Loss: 1.8952\n",
      "Epoch [134], Loss: 1.8951\n",
      "Epoch [135], Loss: 1.8947\n",
      "Epoch [136], Loss: 1.8943\n",
      "Epoch [137], Loss: 1.8947\n",
      "Epoch [138], Loss: 1.8938\n",
      "Epoch [139], Loss: 1.8944\n",
      "Epoch [140], Loss: 1.8941\n",
      "Epoch [141], Loss: 1.8939\n",
      "Epoch [142], Loss: 1.8934\n",
      "Epoch [143], Loss: 1.8935\n",
      "Epoch [144], Loss: 1.8926\n",
      "Epoch [145], Loss: 1.8925\n",
      "Epoch [146], Loss: 1.8928\n",
      "Epoch [147], Loss: 1.8921\n",
      "Epoch [148], Loss: 1.8927\n",
      "Epoch [149], Loss: 1.8920\n",
      "Epoch [150], Loss: 1.8920\n",
      "Epoch [151], Loss: 1.8918\n",
      "Epoch [152], Loss: 1.8961\n",
      "Epoch [153], Loss: 1.8906\n",
      "Epoch [154], Loss: 1.8905\n",
      "Epoch [155], Loss: 1.8907\n",
      "Epoch [156], Loss: 1.8909\n",
      "Epoch [157], Loss: 1.8907\n",
      "Epoch [158], Loss: 1.8905\n",
      "Epoch [159], Loss: 1.8897\n",
      "Epoch [160], Loss: 1.8898\n",
      "Epoch [161], Loss: 1.8893\n",
      "Epoch [162], Loss: 1.8893\n",
      "Epoch [163], Loss: 1.8893\n",
      "Epoch [164], Loss: 1.8902\n",
      "Epoch [165], Loss: 1.8892\n",
      "Epoch [166], Loss: 1.8888\n",
      "Epoch [167], Loss: 1.8896\n",
      "Epoch [168], Loss: 1.8884\n",
      "Epoch [169], Loss: 1.8886\n",
      "Epoch [170], Loss: 1.8890\n",
      "Epoch [171], Loss: 1.8881\n",
      "Epoch [172], Loss: 1.8889\n",
      "Epoch [173], Loss: 1.8874\n",
      "Epoch [174], Loss: 1.8877\n",
      "Epoch [175], Loss: 1.8883\n",
      "Epoch [176], Loss: 1.8907\n",
      "Epoch [177], Loss: 1.8878\n",
      "Epoch [178], Loss: 1.8866\n",
      "Epoch [179], Loss: 1.8868\n",
      "Epoch [180], Loss: 1.8865\n",
      "Epoch [181], Loss: 1.8863\n",
      "Epoch [182], Loss: 1.8880\n",
      "Epoch [183], Loss: 1.8876\n",
      "Epoch [184], Loss: 1.8861\n",
      "Epoch [185], Loss: 1.8859\n",
      "Epoch [186], Loss: 1.8862\n",
      "Epoch [187], Loss: 1.8859\n",
      "Epoch [188], Loss: 1.8872\n",
      "Epoch [189], Loss: 1.8852\n",
      "Epoch [190], Loss: 1.8858\n",
      "Epoch [191], Loss: 1.8853\n",
      "Epoch [192], Loss: 1.8851\n",
      "Epoch [193], Loss: 1.8850\n",
      "Epoch [194], Loss: 1.8849\n",
      "Epoch [195], Loss: 1.8851\n",
      "Epoch [196], Loss: 1.8844\n",
      "Epoch [197], Loss: 1.8849\n",
      "Epoch [198], Loss: 1.8844\n",
      "Epoch [199], Loss: 1.8842\n",
      "Epoch [200], Loss: 1.8842\n",
      "Epoch [201], Loss: 1.8842\n",
      "Epoch [202], Loss: 1.8840\n",
      "Epoch [203], Loss: 1.8846\n",
      "Epoch [204], Loss: 1.8840\n",
      "Epoch [205], Loss: 1.8864\n",
      "Epoch [206], Loss: 1.8831\n",
      "Epoch [207], Loss: 1.8838\n",
      "Epoch [208], Loss: 1.8843\n",
      "Epoch [209], Loss: 1.8835\n",
      "Epoch [210], Loss: 1.8831\n",
      "Epoch [211], Loss: 1.8834\n",
      "Epoch [212], Loss: 1.8835\n",
      "Epoch [213], Loss: 1.8837\n",
      "Epoch [214], Loss: 1.8833\n",
      "Epoch [215], Loss: 1.8864\n",
      "Epoch [216], Loss: 1.8823\n",
      "Epoch [217], Loss: 1.8832\n",
      "Epoch [218], Loss: 1.8825\n",
      "Epoch [219], Loss: 1.8821\n",
      "Epoch [220], Loss: 1.8830\n",
      "Epoch [221], Loss: 1.8830\n",
      "Epoch [222], Loss: 1.8817\n",
      "Epoch [223], Loss: 1.8823\n",
      "Epoch [224], Loss: 1.8838\n",
      "Epoch [225], Loss: 1.8812\n",
      "Epoch [226], Loss: 1.8822\n",
      "Epoch [227], Loss: 1.8839\n",
      "Epoch [228], Loss: 1.8814\n",
      "Epoch [229], Loss: 1.8817\n",
      "Epoch [230], Loss: 1.8813\n",
      "Epoch [231], Loss: 1.8814\n",
      "Epoch [232], Loss: 1.8813\n",
      "Epoch [233], Loss: 1.8808\n",
      "Epoch [234], Loss: 1.8809\n",
      "Epoch [235], Loss: 1.8825\n",
      "Epoch [236], Loss: 1.8818\n",
      "Epoch [237], Loss: 1.8809\n",
      "Epoch [238], Loss: 1.8804\n",
      "Epoch [239], Loss: 1.8814\n",
      "Epoch [240], Loss: 1.8840\n",
      "Epoch [241], Loss: 1.8802\n",
      "Epoch [242], Loss: 1.8896\n",
      "Epoch [243], Loss: 1.8823\n",
      "Epoch [244], Loss: 1.8795\n",
      "Epoch [245], Loss: 1.8799\n",
      "Epoch [246], Loss: 1.8808\n",
      "Epoch [247], Loss: 1.8800\n",
      "Epoch [248], Loss: 1.8803\n",
      "Epoch [249], Loss: 1.8802\n",
      "Epoch [250], Loss: 1.8792\n",
      "Epoch [251], Loss: 1.8808\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[151], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     19\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in seq_dl:\n",
    "        # Forward pass\n",
    "        outputs,hidden = rnn(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print average loss for this epoch\n",
    "    print(f\"Epoch [{epoch+1}], Loss: {total_loss / len(seq_dl):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4e545234-8b79-4f6e-b887-d86f9cb768a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "I am mad   5BheYiiRE1tukogheg,” LE). WheA,”    7 Kée. O,” \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_next_characters(model, seed, char_to_index, index_to_char, max_length=100, temperature=1.0):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Convert the seed to tensor\n",
    "        seed_tensor = torch.tensor([char_to_index[char] for char in seed], dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "        # Initialize the hidden state\n",
    "        hidden = None\n",
    "\n",
    "        # Generate next characters\n",
    "        generated_text = seed\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass\n",
    "            output, hidden = model(seed_tensor,hidden)\n",
    "\n",
    "            # Sample the next character using temperature\n",
    "            probabilities = torch.softmax(output.squeeze() / temperature, dim=0)\n",
    "            next_index = torch.multinomial(probabilities, 1).item()\n",
    "\n",
    "            # Convert the index to character\n",
    "            next_char = index_to_char[next_index]\n",
    "\n",
    "            # Append the next character to the generated text\n",
    "            generated_text += next_char\n",
    "\n",
    "            # Update the seed for the next iteration\n",
    "            seed_tensor = torch.tensor([[next_index]], dtype=torch.long)\n",
    "\n",
    "            # Break if the generated text exceeds max length or if the next character is a newline\n",
    "            if next_char == '\\n' or len(generated_text) >= max_length:\n",
    "                break\n",
    "\n",
    "    # Print the generated text\n",
    "    print(\"Generated text:\")\n",
    "    print(generated_text)\n",
    "\n",
    "seed = \"I am mad\"\n",
    "generate_next_characters(rnn, seed, char_to_idx, idx_to_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb5102d-d876-4613-bd8d-07a081814c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
